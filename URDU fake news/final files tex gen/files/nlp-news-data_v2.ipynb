{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8539408,"sourceType":"datasetVersion","datasetId":5101245}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import make_pipeline\n\n# Load the datasets\nfake_df = pd.read_csv('/kaggle/input/fake-news-dataset/Fake.csv')#[['text', 'label']]\ntrue_df = pd.read_csv('/kaggle/input/fake-news-dataset/True.csv')#[['text', 'label']]\n\n# Add 'label' column and assign values\nfake_df['label'] = 0  # 0 for fake news\ntrue_df['label'] = 1  # 1 for true news\n\n# Select only the 'text' and 'label' columns\nfake_df = fake_df[['text', 'label']]\ntrue_df = true_df[['text', 'label']]\n\n# Combine the datasets\ncombined_df = pd.concat([fake_df, true_df], ignore_index=True)\n\n# Shuffle the combined dataset\ncombined_df = combined_df.sample(frac=1).reset_index(drop=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T17:29:34.792660Z","iopub.execute_input":"2024-06-10T17:29:34.793547Z","iopub.status.idle":"2024-06-10T17:29:35.981934Z","shell.execute_reply.started":"2024-06-10T17:29:34.793513Z","shell.execute_reply":"2024-06-10T17:29:35.981138Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"combined_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:29:38.310357Z","iopub.execute_input":"2024-06-10T17:29:38.311202Z","iopub.status.idle":"2024-06-10T17:29:38.317487Z","shell.execute_reply.started":"2024-06-10T17:29:38.311161Z","shell.execute_reply":"2024-06-10T17:29:38.316402Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(44898, 2)"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Shuffle the combined dataset\ncombined_df = combined_df.sample(frac=1).reset_index(drop=True)\ncombined_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:48:08.136797Z","iopub.execute_input":"2024-06-10T16:48:08.137190Z","iopub.status.idle":"2024-06-10T16:48:08.154767Z","shell.execute_reply.started":"2024-06-10T16:48:08.137152Z","shell.execute_reply":"2024-06-10T16:48:08.153795Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  WASHINGTON (Reuters) - U.S. President-elect Do...      1\n1  WASHINGTON (Reuters) - Congress should launch ...      1\n2  As Barack Hussein Obama tours around the count...      0\n3  John Oliver absolutely humiliated Mitch McConn...      0\n4  It s no secret Americans have been demonstrati...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WASHINGTON (Reuters) - U.S. President-elect Do...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WASHINGTON (Reuters) - Congress should launch ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>As Barack Hussein Obama tours around the count...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>John Oliver absolutely humiliated Mitch McConn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It s no secret Americans have been demonstrati...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Check for null values\nnull_counts = combined_df.isnull().sum()\nprint(\"Null Value Counts:\")\nprint(null_counts)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:49:18.170594Z","iopub.execute_input":"2024-06-10T16:49:18.171518Z","iopub.status.idle":"2024-06-10T16:49:18.185906Z","shell.execute_reply.started":"2024-06-10T16:49:18.171485Z","shell.execute_reply":"2024-06-10T16:49:18.184915Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Null Value Counts:\ntext     0\nlabel    0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove rows with null values\ncombined_df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:49:41.421029Z","iopub.execute_input":"2024-06-10T16:49:41.421381Z","iopub.status.idle":"2024-06-10T16:49:41.437501Z","shell.execute_reply.started":"2024-06-10T16:49:41.421353Z","shell.execute_reply":"2024-06-10T16:49:41.436245Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Check if null values are removed\nnull_counts_after = combined_df.isnull().sum()\nprint(\"Null Value Counts After Removing:\")\nprint(null_counts_after)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:49:42.671580Z","iopub.execute_input":"2024-06-10T16:49:42.672328Z","iopub.status.idle":"2024-06-10T16:49:42.686508Z","shell.execute_reply.started":"2024-06-10T16:49:42.672294Z","shell.execute_reply":"2024-06-10T16:49:42.685488Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Null Value Counts After Removing:\ntext     0\nlabel    0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK resources\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Define function for text preprocessing\ndef preprocess_text(text):\n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Lowercasing\n    tokens = [token.lower() for token in tokens]\n    \n    # Remove punctuation and special characters\n    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Join tokens back\n    preprocessed_text = ' '.join(tokens)\n    \n    return preprocessed_text\n\n# Apply preprocessing function to the 'text' column\ncombined_df=combined_df.head(50)\ncombined_df['text'] = combined_df['text'].apply(preprocess_text)\n\n# Print sample preprocessed text\nprint(\"Sample Preprocessed Text:\")\nprint(combined_df['text'].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:55:47.805880Z","iopub.execute_input":"2024-06-10T16:55:47.806699Z","iopub.status.idle":"2024-06-10T16:55:48.089129Z","shell.execute_reply.started":"2024-06-10T16:55:47.806666Z","shell.execute_reply":"2024-06-10T16:55:48.088079Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nSample Preprocessed Text:\n0    washington  reuters   us presidentelect donald...\n1    washington  reuters   congress launch bipartis...\n2    barack hussein obama tours around country tryi...\n3    john oliver absolutely humiliated mitch mcconn...\n4    secret americans demonstrating complete lack i...\nName: text, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:58:11.060639Z","iopub.execute_input":"2024-06-10T16:58:11.061566Z","iopub.status.idle":"2024-06-10T16:58:11.067092Z","shell.execute_reply.started":"2024-06-10T16:58:11.061532Z","shell.execute_reply":"2024-06-10T16:58:11.066172Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(50, 2)"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Drop blank rows if any\ncombined_df.dropna(inplace=True)\n\n# Select a desired number of rows\ndesired_rows = 1000  # Change this to your desired number of rows\ncombined_df_subset = combined_df.head(desired_rows)\n\n# Feature extraction using TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(combined_df_subset['text']).toarray()\n\n# Encode labels from combined_df_subset\nencoder = LabelEncoder()\ny = encoder.fit_transform(combined_df_subset['label'])\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:06:17.659055Z","iopub.execute_input":"2024-06-10T17:06:17.659456Z","iopub.status.idle":"2024-06-10T17:06:17.695497Z","shell.execute_reply.started":"2024-06-10T17:06:17.659426Z","shell.execute_reply":"2024-06-10T17:06:17.694602Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":" # adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report\n\n# Initialize and train AdaBoost classifier\nada = AdaBoostClassifier(n_estimators=100)\nada.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = ada.predict(X_test)\nprint(\"AdaBoost Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:06:27.201788Z","iopub.execute_input":"2024-06-10T17:06:27.202170Z","iopub.status.idle":"2024-06-10T17:06:27.223283Z","shell.execute_reply.started":"2024-06-10T17:06:27.202141Z","shell.execute_reply":"2024-06-10T17:06:27.222481Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"AdaBoost Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         6\n           1       1.00      1.00      1.00         4\n\n    accuracy                           1.00        10\n   macro avg       1.00      1.00      1.00        10\nweighted avg       1.00      1.00      1.00        10\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize and train GBM classifier\ngbm = GradientBoostingClassifier(n_estimators=100)\ngbm.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = gbm.predict(X_test)\nprint(\"GBM Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:06:34.251330Z","iopub.execute_input":"2024-06-10T17:06:34.251693Z","iopub.status.idle":"2024-06-10T17:06:34.452422Z","shell.execute_reply.started":"2024-06-10T17:06:34.251667Z","shell.execute_reply":"2024-06-10T17:06:34.451441Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"GBM Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         6\n           1       1.00      1.00      1.00         4\n\n    accuracy                           1.00        10\n   macro avg       1.00      1.00      1.00        10\nweighted avg       1.00      1.00      1.00        10\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# XGBM\nimport xgboost as xgb\n\n# Initialize and train XGBoost classifier\nxgb_model = xgb.XGBClassifier(n_estimators=100)\nxgb_model.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = xgb_model.predict(X_test)\nprint(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:06:38.574307Z","iopub.execute_input":"2024-06-10T17:06:38.575140Z","iopub.status.idle":"2024-06-10T17:06:38.914234Z","shell.execute_reply.started":"2024-06-10T17:06:38.575107Z","shell.execute_reply":"2024-06-10T17:06:38.913320Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"XGBoost Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         6\n           1       1.00      1.00      1.00         4\n\n    accuracy                           1.00        10\n   macro avg       1.00      1.00      1.00        10\nweighted avg       1.00      1.00      1.00        10\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# CNN 4 layers\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n\n# Tokenizing and padding sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(combined_df['text'])\nX_seq = tokenizer.texts_to_sequences(combined_df['text'])\nX_pad = pad_sequences(X_seq, maxlen=100)\n\n# Train-test split for CNN\nX_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n\n# Build 4-layer CNN model\nmodel_4 = Sequential([\n    Embedding(input_dim=5000, output_dim=128, input_length=100),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_4.summary()\n\n# Train the model\nmodel_4.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\nloss, accuracy = model_4.evaluate(X_test, y_test)\nprint(f'4-Layer CNN Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:08:20.205175Z","iopub.execute_input":"2024-06-10T17:08:20.206151Z","iopub.status.idle":"2024-06-10T17:08:28.194327Z","shell.execute_reply.started":"2024-06-10T17:08:20.206116Z","shell.execute_reply":"2024-06-10T17:08:28.193404Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4375 - loss: 0.6942","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718039305.345994     544 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.4375 - loss: 0.6942 - val_accuracy: 0.7500 - val_loss: 0.6055\nEpoch 2/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5625 - loss: 0.6116 - val_accuracy: 0.7500 - val_loss: 0.5978\nEpoch 3/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9688 - loss: 0.4983 - val_accuracy: 0.7500 - val_loss: 0.6037\nEpoch 4/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9688 - loss: 0.3828 - val_accuracy: 0.7500 - val_loss: 0.5990\nEpoch 5/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9688 - loss: 0.2802 - val_accuracy: 0.7500 - val_loss: 0.5786\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.5000 - loss: 0.6719\n4-Layer CNN Accuracy: 0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"# CNN 6 layers\n# Build 6-layer CNN model\nmodel_6 = Sequential([\n    Embedding(input_dim=5000, output_dim=128, input_length=100),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Conv1D(64, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_6.summary()\n\n# Train the model\nmodel_6.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\nloss, accuracy = model_6.evaluate(X_test, y_test)\nprint(f'6-Layer CNN Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:08:54.426110Z","iopub.execute_input":"2024-06-10T17:08:54.426808Z","iopub.status.idle":"2024-06-10T17:09:00.923975Z","shell.execute_reply.started":"2024-06-10T17:08:54.426774Z","shell.execute_reply":"2024-06-10T17:09:00.923091Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.6935 - val_accuracy: 0.7500 - val_loss: 0.6838\nEpoch 2/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8125 - loss: 0.6719 - val_accuracy: 0.7500 - val_loss: 0.6682\nEpoch 3/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6875 - loss: 0.6594 - val_accuracy: 0.7500 - val_loss: 0.6547\nEpoch 4/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7500 - loss: 0.6363 - val_accuracy: 0.7500 - val_loss: 0.6384\nEpoch 5/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6875 - loss: 0.5899 - val_accuracy: 0.7500 - val_loss: 0.6256\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 541ms/step - accuracy: 0.6000 - loss: 0.6595\n6-Layer CNN Accuracy: 0.6000000238418579\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text GAN\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom IPython.display import display\n\n\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n\ndef load_datasets(true_path, fake_path):\n    texts =combined_df['text']\n    labels = combined_df['label']\n    return texts, labels\n\ndef create_dataloader(texts, tokenizer, max_length, batch_size):\n    dataset = TextDataset(texts, tokenizer, max_length)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndef generate_text_gpt2(model, tokenizer, prompt, max_new_tokens=50):\n    try:\n        inputs = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=True, max_length=1024, truncation=True)\n        attention_mask = torch.ones(inputs.shape)\n        \n        # Check tensor dimensions and values before moving to device\n        if inputs.shape[1] > 1024:\n            raise ValueError(f\"Input sequence length exceeds the maximum length of 1024 tokens: {inputs.shape[1]}\")\n        \n        inputs = inputs.to(device)\n        attention_mask = attention_mask.to(device)\n        \n        outputs = model.generate(inputs, attention_mask=attention_mask, max_new_tokens=max_new_tokens, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n    except Exception as e:\n        print(f\"Error generating text for input: {prompt[:50]}...: {e}\")\n        return None\n\ndef generate_and_add_texts_to_dataset(model, tokenizer, texts, labels, label, max_new_tokens=50, subset_size=100):\n    new_texts = []\n    with ThreadPoolExecutor() as executor:\n        futures = [\n            executor.submit(generate_text_gpt2, model, tokenizer, text, max_new_tokens)\n            for text in texts[:subset_size]\n        ]\n        for idx, future in enumerate(as_completed(futures)):\n            try:\n                generated_text = future.result()\n                if generated_text:\n                    new_texts.append(generated_text)\n                    labels.append(label)\n                else:\n                    print(f\"Skipped text at index {idx} due to generation error.\")\n            except Exception as e:\n                print(f\"Error processing text at index {idx}: {e}\")\n    texts.extend(new_texts)\n    return texts, labels\n\nif __name__ == \"__main__\":\n    try:\n        # Paths to the datasets\n        true_path = '/kaggle/input/fake-news-dataset/True.csv'\n        fake_path = '/kaggle/input/fake-news-dataset/Fake.csv'\n\n        # Load datasets\n        texts, labels = load_datasets(true_path, fake_path)\n\n        # Initialize tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        max_length = 128\n\n        # Create DataLoader\n        batch_size = 16\n        dataloader = create_dataloader(texts, tokenizer, max_length, batch_size)\n\n        # Ensure using CPU for debugging\n        device = torch.device('cpu')\n        print(f\"Using device: {device}\")\n\n        # Initialize the model\n        model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n        model.eval()\n\n        # Process a controlled subset of data\n        subset_size =  100 # Process only 100 records for faster execution\n\n        # Generate new texts based on original texts and add them to the dataset\n        texts, labels = generate_and_add_texts_to_dataset(model, tokenizer, texts, labels, label=1, max_new_tokens=50, subset_size=subset_size)\n        texts, labels = generate_and_add_texts_to_dataset(model, tokenizer, texts, labels, label=0, max_new_tokens=50, subset_size=subset_size)\n\n        # Verify the new dataset size\n        print(f\"Total texts: {len(texts)}\")\n        print(f\"Total labels: {len(labels)}\")\n\n        # Create DataLoader with updated dataset\n        dataloader = create_dataloader(texts, tokenizer, max_length, batch_size)\n\n        # Convert to DataFrame to display\n        df = pd.DataFrame({'text': texts, 'label': labels})\n        display(df.head())  # Display the first few rows of the updated dataset\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:35:32.327236Z","iopub.execute_input":"2024-06-10T17:35:32.328007Z","iopub.status.idle":"2024-06-10T17:35:51.633940Z","shell.execute_reply.started":"2024-06-10T17:35:32.327977Z","shell.execute_reply":"2024-06-10T17:35:51.625369Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Using device: cpu\nError processing text at index 0: 'Series' object has no attribute 'append'\nError processing text at index 1: 'Series' object has no attribute 'append'\nError processing text at index 2: 'Series' object has no attribute 'append'\nError processing text at index 3: 'Series' object has no attribute 'append'\nError generating text for input: NAIROBI/MOGADISHU (Reuters) - The size and methods...: index out of range in self\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 71\u001b[0m, in \u001b[0;36mgenerate_and_add_texts_to_dataset\u001b[0;34m(model, tokenizer, texts, labels, label, max_new_tokens, subset_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     68\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(generate_text_gpt2, model, tokenizer, text, max_new_tokens)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts[:subset_size]\n\u001b[1;32m     70\u001b[0m ]\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(as_completed(futures)):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Process only 100 records for faster execution\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Generate new texts based on original texts and add them to the dataset\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m texts, labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_and_add_texts_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m texts, labels \u001b[38;5;241m=\u001b[39m generate_and_add_texts_to_dataset(model, tokenizer, texts, labels, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, subset_size\u001b[38;5;241m=\u001b[39msubset_size)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Verify the new dataset size\u001b[39;00m\n","Cell \u001b[0;32mIn[75], line 66\u001b[0m, in \u001b[0;36mgenerate_and_add_texts_to_dataset\u001b[0;34m(model, tokenizer, texts, labels, label, max_new_tokens, subset_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_and_add_texts_to_dataset\u001b[39m(model, tokenizer, texts, labels, label, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, subset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     65\u001b[0m     new_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     67\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     68\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(generate_text_gpt2, model, tokenizer, text, max_new_tokens)\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts[:subset_size]\n\u001b[1;32m     70\u001b[0m         ]\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(as_completed(futures)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"Error generating text for input: Will this FINALLY be the straw that breaks the cam...: index out of range in self\nError generating text for input: THIS IS SO IMPORTANT! The transcript and video  be...: index out of range in self\nError generating text for input:  In response to the establishment media s contrive...: index out of range in self\nError generating text for input: ANKARA/BEIRUT (Reuters) - Syrian rebel fighters ar...: index out of range in self\nError generating text for input: How is a man with ties to a US based terror organi...: index out of range in self\nError generating text for input: BRUSSELS (Reuters) - When Theresa May visits Bruss...: index out of range in self\nError generating text for input: (Reuters) - Following is the full text of former F...: index out of range in self\nError generating text for input: Everyone suspected the sketchy Steele Dossier was ...: index out of range in self\n","output_type":"stream"}]},{"cell_type":"code","source":"# newly generated data\n\ndf = df.sample(frac=1).reset_index(drop=True)\ndf['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:34:47.986977Z","iopub.execute_input":"2024-06-10T17:34:47.987742Z","iopub.status.idle":"2024-06-10T17:34:47.995743Z","shell.execute_reply.started":"2024-06-10T17:34:47.987711Z","shell.execute_reply":"2024-06-10T17:34:47.994802Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"array([1])"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Drop blank rows if any\ndf.dropna(inplace=True)\n\n# Select a desired number of rows\ndesired_rows = 1000  # Change this to your desired number of rows\ndf = df.head(desired_rows)\n\n# Feature extraction using TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(df['text']).toarray()\n\n# Encode labels from combined_df_subset\nencoder = LabelEncoder()\ny = encoder.fit_transform(df['label'])\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:34:14.365675Z","iopub.execute_input":"2024-06-10T17:34:14.366581Z","iopub.status.idle":"2024-06-10T17:34:14.831326Z","shell.execute_reply.started":"2024-06-10T17:34:14.366547Z","shell.execute_reply":"2024-06-10T17:34:14.830497Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:34:21.057525Z","iopub.execute_input":"2024-06-10T17:34:21.058228Z","iopub.status.idle":"2024-06-10T17:34:21.067566Z","shell.execute_reply.started":"2024-06-10T17:34:21.058197Z","shell.execute_reply":"2024-06-10T17:34:21.066599Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"code","source":"# adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report\n\n# Initialize and train AdaBoost classifier\nada = AdaBoostClassifier(n_estimators=100)\nada.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = ada.predict(X_test)\nprint(\"AdaBoost Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:33:02.202636Z","iopub.execute_input":"2024-06-10T17:33:02.203360Z","iopub.status.idle":"2024-06-10T17:33:02.236286Z","shell.execute_reply.started":"2024-06-10T17:33:02.203328Z","shell.execute_reply":"2024-06-10T17:33:02.235338Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"AdaBoost Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       200\n\n    accuracy                           1.00       200\n   macro avg       1.00      1.00      1.00       200\nweighted avg       1.00      1.00      1.00       200\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize and train GBM classifier\ngbm = GradientBoostingClassifier(n_estimators=100)\ngbm.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = gbm.predict(X_test)\nprint(\"GBM Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:33:05.791087Z","iopub.execute_input":"2024-06-10T17:33:05.791686Z","iopub.status.idle":"2024-06-10T17:33:05.875654Z","shell.execute_reply.started":"2024-06-10T17:33:05.791652Z","shell.execute_reply":"2024-06-10T17:33:05.874509Z"},"trusted":true},"execution_count":69,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize and train GBM classifier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m gbm \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predictions and evaluation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gbm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:440\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    437\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 440\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_y(y)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:1232\u001b[0m, in \u001b[0;36mGradientBoostingClassifier._validate_y\u001b[0;34m(self, y, sample_weight)\u001b[0m\n\u001b[1;32m   1230\u001b[0m n_trim_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(np\u001b[38;5;241m.\u001b[39mbincount(y, sample_weight))\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_trim_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class after sample_weight \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrimmed classes with zero weights, while a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimum of 2 classes are required.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m n_trim_classes\n\u001b[1;32m   1236\u001b[0m     )\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# expose n_classes_ attribute\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required."],"ename":"ValueError","evalue":"y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.","output_type":"error"}]},{"cell_type":"code","source":"# XGBM\nimport xgboost as xgb\n\n# Initialize and train XGBoost classifier\nxgb_model = xgb.XGBClassifier(n_estimators=100)\nxgb_model.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = xgb_model.predict(X_test)\nprint(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:31:03.751228Z","iopub.status.idle":"2024-06-10T16:31:03.751548Z","shell.execute_reply.started":"2024-06-10T16:31:03.751394Z","shell.execute_reply":"2024-06-10T16:31:03.751407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN 4 layers\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n\n# Tokenizing and padding sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(df['text'])\nX_seq = tokenizer.texts_to_sequences(df['text'])\nX_pad = pad_sequences(X_seq, maxlen=100)\n\n# Train-test split for CNN\nX_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n\n# Build 4-layer CNN model\nmodel_4 = Sequential([\n    Embedding(input_dim=5000, output_dim=128, input_length=100),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_4.summary()\n\n# Train the model\nmodel_4.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\nloss, accuracy = model_4.evaluate(X_test, y_test)\nprint(f'4-Layer CNN Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:31:03.754271Z","iopub.status.idle":"2024-06-10T16:31:03.754580Z","shell.execute_reply.started":"2024-06-10T16:31:03.754429Z","shell.execute_reply":"2024-06-10T16:31:03.754442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN 6 layers\n# Build 6-layer CNN model\nmodel_6 = Sequential([\n    Embedding(input_dim=5000, output_dim=128, input_length=100),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Conv1D(64, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_6.summary()\n\n# Train the model\nmodel_6.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\nloss, accuracy = model_6.evaluate(X_test, y_test)\nprint(f'6-Layer CNN Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:31:03.758133Z","iopub.status.idle":"2024-06-10T16:31:03.758451Z","shell.execute_reply.started":"2024-06-10T16:31:03.758297Z","shell.execute_reply":"2024-06-10T16:31:03.758310Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Error generating text for input: WASHINGTON (Reuters) - The U.S. Supreme Court on M...: index out of range in self\nError generating text for input: WASHINGTON/NEW YORK (Reuters) - A series of tweets...: index out of range in self\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}